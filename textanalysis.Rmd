#Setting up
## load required R packages
```{r}
packages = c('tidyverse', 'tidytext', 'tm', 'SnowballC', 'wordcloud', 'wordcloud2', 'RColorBrewer', 'stringi', 'readr', 'stringr', 'topicmodels', 'ggplot2', 'dplyr', 'plotly', 'parallelPlot', 'readtext', 'quanteda', 'stringi', 'sentimentr', 'SentimentAnalysis', 'syuzhet', 'RSentiment')
for (p in packages){
if(!require(p, character.only = T)){
install.packages(p)
}
library(p, character.only = T)
}
```

## read in Airbnb.csv
```{r}
airbnb <- read_csv("data/Airbnb_victoria.csv")
```
# Preparing the data
## tokenisation to create a tibble of text from 'description' column
```{r}
airbnb_desc <- airbnb$description
airbnb_text <- iconv(airbnb_desc, to = "UTF-8")
airbnb_tibble <- tibble(airbnb_text)
airbnb_tibble %>%
  unnest_tokens(word, airbnb_text)
```

## Create a corpus and tokenisation
```{r}
# creating the corpus
airbnb_corpus <- Corpus(VectorSource(airbnb_tibble))

# tokenisation
airbnb_corpus_clean <- tm_map(airbnb_corpus, stripWhitespace)
airbnb_corpus_clean <- tm_map(airbnb_corpus_clean, content_transformer(tolower))
airbnb_corpus_clean <- tm_map(airbnb_corpus_clean, removeNumbers)
airbnb_corpus_clean <- tm_map(airbnb_corpus_clean, removePunctuation)
airbnb_corpus_clean <- tm_map(airbnb_corpus_clean, removeWords, c((stopwords("english")), "%br"))
airbnb_corpus_clean <- tm_map(airbnb_corpus_clean, stemDocument)
```
#create the Document Term Matrix
```{r}
airbnb_corpus_dtm <- DocumentTermMatrix(airbnb_corpus_clean)
```

#Creating the visualisations

## Wordcloud
```{r}
sums <- as.data.frame(colSums(as.matrix(airbnb_corpus_dtm)))
sums <- rownames_to_column(sums) 
colnames(sums) <- c("terms", "count")
sums <- arrange(sums, desc(count))
head <- sums

#wordcloud will show the most common terms used in the 'description' column
wordcloud(words = head$terms, freq = head$count, min.freq = 100,
  max.words=1000, random.order=FALSE, rot.per=0.35, 
  colors=brewer.pal(8, "Dark2"))
```

## Latent Dirichlet Allocation (LDA) model
```{r}
# set a seed so that the output of the model is predictable
airbnb_lda <- LDA(airbnb_corpus_dtm, k = 5)
#> A LDA_VEM topic model with 5 topics.
```

## extracting the per-topic-per-word probabilities, called Î² (beta)
```{r}
airbnb_topics <- tidy(airbnb_lda, matrix = "beta")
```

## visualise the topics
```{r}
## find the 10 most common terms within each topic
airbnb_top_terms <- airbnb_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

## visualising the output
airbnb_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()
```