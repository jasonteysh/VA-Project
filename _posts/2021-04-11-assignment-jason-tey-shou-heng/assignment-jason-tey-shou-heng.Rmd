---
title: "Assignment - Jason Tey"
description: |
  This post presents a literature review, recommendations, and prototype of proposed modules to fill existing gaps in analytical tools by enhancing user experience using a web-based application.
  
  This post constitutes part of the assessment for credit towards the Singapore Management Univerisity module ISSS608 Visual Analytics and Application, AY2020/2021 Term 2. 
author:
  - name: Jason Tey
    url: {https://jason-tey.netlify.app/}
date: 4-11-2021
output:
  distill::distill_article:
    toc: true 
    self_contained: false
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, results='hide', message=FALSE, error=FALSE, warning=FALSE)
```



# 1: Introduction

This section outlines the objective of this project and provides a literature review on how analysis are currently performed, what the existing gaps are, and recommends enhancements to better user experience on using the analysis techniques.  

## 1.1 Objective

This post forms part of a larger visual analytics project that aims to develop a web-based application to enhance user experience in employing analysis techniques. The analytics project aims to develop user-friendly analytical tools in the following areas:  

- Exploratory Data Analysis
- Clustering Analysis 
- Geospatial Data Analysis 
- Text Analysis 
- Predictive Analysis 

The project employs listings data scrapped from the Airbnb website, made available by Inside AirBnb via this [link](http://insideairbnb.com/get-the-data.html). The listings data from Australia (listings.csv) compiled by the website on 22 January 2021 was used for the purpose of this project.  

This post focuses on the discussion of the **Geospatial Data Visualization and Analysis** modules of the web-based application. In summary, a literature review was conducted to research on existing efforts in this aspect; gaps in existing efforts were then identified; enhancements based on well-studied alternatives were proposed; and finally, prototypes of the recommended analytical tools were developed, tested, and discussed in this post.  

## 1.2 Literature Review

The data offered by Inside Airbnb are split into several different data sets - this assignment will be focusing on the listings file, which provides detailed data from individual listings put up on the AirBnb database. Information provided includes hosts details, response data, listing information (e.g. price, number of beds, property type, maximum nights), and review scores.  

There are currently a number of analyses that has been conducted on the Airbnb dataset. While most of them are focused on one particular country, given that the datasets made available by Inside Airbnb are rather consistent across different geographical space, it is reasonable to assume that the analytical methodologies adopted by these other studies can be easily replicated and reproduced with data from other countries.  

One of most prominent and recent study on Airbnb data is by Steve Deane from Stratos, who wrote a [blogpost in January 2021](https://www.stratosjets.com/blog/airbnb-statistics/) on the topic (which also inspired this project). In his post, Deane provides descriptive statistics (e.g. total number of hosts, most popular destination, number of guests) focusing on the global distribution with a slight inclination towards the United States of America. While some of the statistics were provided, Deane's analysis is heavily weighted towards the economic aspects of Airbnb (e.g. whether Airbnb affects property values and rents - "Airbnb effect", whether Airbnb is cheaper than hotels, and economic impact of Airbnb in each country).  However, Deane does not provide any higher level data analysis beyond the descriptive ones. He did presented one segment explaining the factors that host should consider when purchasing a property to use as an Airbnb - reflecting some form of explanatory analysis (and perhaps even predictive) but the content was scarce with no elaboration on how these factors were derived. The major 'flaw', though, remains the fact that Deane's blogpost is heavy on qualitative write-up with minimal (or no) meaningful visualization of the statistics he has quoted.  

Several blogposts on medium.com provided basic guides on data analysis on the Airbnb dataset. [Kwon et. al. (2018)](https://medium.com/@jkwonhalla/airbnb-listings-data-analysis-with-r-e2c6001147a6#:~:text=%20Airbnb%20Listings%20Data%20Analysis%20with%20R%20%28Analyses%29,from%20applying%20LDA%20to%20the%20three-class...%20More%20), [Chen (2019)](https://medium.com/analytics-vidhya/how-to-analyze-airbnb-performance-data-in-the-right-way-b83f3dad1458), and [Gedik (2020)](https://medium.com/@emrebilgehangedik/seattle-airbnb-listings-analysis-d88c839596f8) are some recent examples.  

Kwon et. al. (2018) used the Inside Airbnb listing data from Austin, Texas with around 12,ooo listings, and utilized the numerical features to apply Linear Discriminant Analysis, Outliers were detected and removed using the Cook's distance before a Box-Cox transformation was conducted to normalize the data, and the dependent variable (review score) binned to wrangle into categorical data type. Backward Elimination, Ordinal Logistic Regression, and LASSO Regression methods were employed to conduct explanatory analysis - with all three models turning number of bathrooms as significant predictor for customer ratings. Principal Component Analysis was then conducted with no good result after the model fail to meaningfully provide separation of classes (three classes). The Latent Discriminant Analysis was then conducted on each of the three classes resulted from the PCA. The study concluded that number of listings by hosts and having more bathrooms are crucial in securing higher review score. The advantage of this study is the depth, with detailed discussion on the methodology and results. However, the analysis focuses only on predictive/explanatory model, and is one-dimensional, studying only one dependent variable. The study does not offer interactive features to allow other forms of data exploration (e.g. other clustering methods, geographical influence) or changing of dependent variable (e.g. to examine factors affecting price of listing).  

Chen (2019) on the other hand provides an analysis that emphasized on the geographical distribution of listings, and provided more data visualizations (playing to the advantage of using Tableau) that allow viewers to make quick noticeable trends. Chen analysed the 2019 New York City listing data with the objective to predict future Airbnb performance in the city. However, while a predictive model was envisioned, there was little predictive analysis going on in her study - a greater emphasis was placed on qualitative inferences made via the visualization without proper data analytics methodologies. Chen was also limited in her methods of data visualization - for example, when presenting the geospatial distribution of areas by price, she had used a proportional dot map, without a convincing argument of its strength as compared to other visualization methods (such as a choropleth). In fact, the proportional size between the different dots are difficult to be differentiated based on her viz. There are also major flaws with other visualization choice (e.g. showing Average Price by Locations with equal-length bar differentiated by colors on a continuous scale), and the usefulness of some of the viz is also questionable (why might we be interested to know who are the Top 10 Busy Host, and indeed, what is the definition of 'business' here). All in all, while Chen presented some simple visualization to expose the potential of data viz using the Airbnb data, more is left desired from the analysis.  

Comparatively, Gedik (2020) did a fair job in answering the questions set out by his objective. Using the Seattle listing dataset (via Kaggle), he aimed to find out the common amenities, and top features attracting guests and higher prices. In a Question-and-Answer format, Gedik presented visualizations to help answer each of the question he asked in a simple and succinct manner. In his third question, Gedik also presented the result table of a linear regression model he had ran, showing that property_type_Boat has the largest effect on price of listing. Gedik's post, however, suffers from the limitation of scarce discussion in the technical methodology aspect. Similar to earlier studies, there is also no interactivity offered to allow viewers autonomy in changing parameters.  

Amongst all, [Gupta (2019)](https://towardsdatascience.com/airbnb-rental-listings-dataset-mining-f972ed08ddec) provided the most well-rounded discussion and presentation using the Airbnb listing data. Gupta aimed to provide an exploratory analysis of Airbnb's data to understand the rental landscape in New York City. Gupta first employed descriptive time-series statistics to map out the increasing trends in number of listings and reviews in the city, before moving on to present an interactive Shiny App that provides information on individual listing based on sets of filters (e.g. max budget, number of people, minimum rating). While Gupta offered some form of interactivity, the Shiny does not provide any meaningful insights, and is essentially a replica of the user interface offered by Airbnb via the official website.   

While non-interactive, Gupta's subsequent discussion provides a preliminary view of the analytical methodologies that could be employed using the Inside Airbnb listing data. Firstly, he mapped out a geographical spread of the ratings and price by area using a Choropleth map (subsequent discussion in this paper will also conclude that it is indeed one of the better geospatial visualization option for this application). He also provided a quick bar-chart viz of the breakdown in terms of property types, before moving on to discuss the temporal nature and seasonality effect of price in New York City (using both a time series dot plot to analyse monthly patterns and a boxplot for each day of the week. He even employed a calendar heat map of occupancy within the city. After geospatial and temporal analysis, Gupta moved on to conduct text analytics on the reviews dataset to find words most commonly mentioned - an attempt to distil aspects that play important roles in shaping the Airbnb experience. A Shiny app was also developed to allow user to find similar word vectors based on a query word. Lastly, Gupta set out to find out whether there are any correlation between the host response rate, the average ratings, and whether the host is a Superhost. While no predictive model was used, the scatter plot presented provides a quick viz on the explanatory analysis he desired to conduct.  

Within the industry, there exists interactive tools that allow analysts or potential hosts to analyze rental data using attributes and past performance. One such example is [AIRDNA](https://www.airdna.co/). Notwithstanding the fact that the platform only offers paid services, the results are also provided in a prescriptive manner with little analytical value-add.  


## 1.3 Existing Gaps & Proposed Enhancement

Taking in all the existing literature availble on the topic, the following gaps were identified.  


**Existing Gaps**


| S/N | Gaps | Elaboration                                                              |
|-----|-------|-------------------|
| 1   | Lack of coordinated in-depth analysis | All of the studies discussed earlier focused on single dimensional aspect of the dataset - for example, Kwon (2018) on predicting ratings, Chen (2019) on visualizing the data, and Gedik (2020) on identifying common amenities potentially useful in predicting demand and price. The only attempt at merging the analyses into a centralised platform to be used for thematic analysis is Gupta's. Notwithstanding the fact that his Shiny apps are hosted on separate io pages, Gupta's analyses are also disjointed in content. Gupta started off providing descriptive analysis of Airbnb in New York, then transited to a Shiny for property locator, then more exploratory analysis on geospatial and categortical distribution, before transiting to temporal and text analysis - throughout his study, there is no one common theme that links the different methodologies and discussion together. A more concerted and systematic organization of the analysis and the tools used is needed.|
| 2   | No provision of analytical tool to help viewers do the analysis themselves| Across all the studies examined, including Gupta's Shiny app, there are no provision of analytical tool that can allow users to change certain parameters to tweak the analyses to their own objective/preference. For example, in Kwon's predictive modelling, there is no way to change the methods or predictor that is included, or perhaps even to change the dependent variable of interest. In Chen's visualization, there is also no way to change the variable presented, say from her original price-as-size, to examine ratings using size or even colors on the viz. Similarly, Gedik does not offer any tools in this aspect. Gupta, while offering a wide range of analytical methods and even offering Shiny app, also surprisingly fell short in this area - for example, in his text analytics app, while user can change the word query, they are unable to, say, focus on reviews with more than a 4-star ratings to find out common words associated with higher ratings. |
| 3   | Lack of Interactivity| Another gap that is found across all study and one of the most major flaw considering the wealth of information provided by the datasets. None of the study offered options to, for example, zoom in on a particular sub-region, or to filter out whether a hosts is a superhost, or to subset data based on a range of prices. These are useful features that could tremendously enhance the usefulness of the analysis |

**Proposed Enhancements**

| S/N | Gaps | Proposed Enhancement                                                        |
|-----|-------|-------------------|
| 1   | Lack of coordinated in-depth analysis | The analyses and platform offered should allow user to **conduct holistic analyses within a coordinated theme using a variety of analytical tools**. For example, in analyzing the prices for Airbnb listing, exploratory analyses can be conducted to filter out differences or relationships between price and other variables; geospatial distribution can be mapped out to identify geographical clusters or outliers of price; predictive analysis can be adopted to distill factors affecting prices; and text analysis can be employed on reviews or descriptions to find words commonly associated with higher prices, This way, the application provides a robust selection of analytical tools that user can utilize to derive coherent views about a single question asked, in contrast to the haphazard loading of different analytical methodologies that results in a messy paper with no clear objectives presented by the studies discussed earlier. |
| 2   | No provision of analytical tool to help viewers do the analysis themselves| Platform and tool developed should provide user with easy-to-use toggles and filters to tweak the analyses to their desire and preference. For example, the dependent variable of price as discussed in s/n 1 above can be easily changed to ratings or response rate to understand other dimensions of the listing data. This will be more useful than to present a static version of analysis, while also allowing analysts without coding to easily conduct appropriate analysis  |
| 3   | Lack of Interactivity| Lastly, interactivity should be introduced to enhance information transmission. Beyond the toggles/filters that allow user to vary relevant parameters, visualization should also make use of interactivity (e.g. tool tips on hover) to provide richer information to user/viewer, that can better enhance user experience of the application. |  


After examining some of the existing gaps in the form of data analysis, visualization and dashboarding in general for similar dataset used in the field, the next segment focuses on the storyboarding for the geospatial analysis aspect that is the focus of this sub-module/assignment. Further discussions on some of the literature on appropriate visualization and analytical tool for the geospatial analysis can be found in Section 2 below.  


# 2: Storyboard  

## 2.1 Sub-module 1: Geospatial Visualization  

There are several commonly-used geospatial visualization techniques available, these include Dot Density Map, Proportional Symbols, Cartograms, and Choropleth maps.  


**Dot Density Map**  

The [dot density maps](http://www.axismaps.com.s3-website-us-east-1.amazonaws.com/guide/univariate/dot-density/) is useful in showing "*differences in geographic distributions across a landscape*".  

1. Raw data and simple counts or rates/ratio can all be mapped 
2. Data need not be tied to enumeration units (that is, it does not require the variable of interest to be conceptually measurable anywhere in space)
3. It works fine with black and white when color is not an option (and if color is available, it can be used to represent other variable of interest)

In parallel, difficulty in retrieving exact number from the viz is also one major limitation of dot density maps - viewers generally will not be patient to manually count the number of dots in a region to derive the total number of events in the particular space.If the purpose of the visualization is for data extraction and to allow accurate extration of aggregated numbers within an area, dot density may not be the optimal choice.  

**Proportional Symbols**  

[Proportional symbol maps](http://www.axismaps.com.s3-website-us-east-1.amazonaws.com/guide/univariate/proportional-symbols/) scale the size of simple symbols (usually a circle or square) proportionally to the data value found at that location. Proportional symbols can be used for numerical data or ordinal categorical data. When aggregated, proportional symbol overcomes the limitation of dot density by allowing comparison of variable using relative size - i.e. larger the symbol, more of the variable of interest.  

In fact, proportional symbols can be complemented with dot density (and Choropleth, discussed below) to provide an additional dimension to the visualization (i.e. dot represents one count e.g. one instance of listing; size representing one variable of interest e.g. price; and in fact colors or other element such as secondary shape (outer circle) can be deployed to add even more dimension to the viz - though running the risk of over-complicating and/or over-crowding the map with excessive info).  

One common problem with proportional symbol map, as mentioned above, is congestion, especially if there are large variations in size and/or data points are close together. Another limitation is that readers generally do no estimate the areas of symbols very well.  

**Cartogram**

Amongst the various viz methods, the [Cartograms](http://www.axismaps.com.s3-website-us-east-1.amazonaws.com/guide/univariate/cartograms/) is arguably the least popular, as "[their use is often more for their theatrical value (i.e. to grab the attention of an audience) and less for their ability to help folks understand subtle details of geographic datasets](http://www.axismaps.com.s3-website-us-east-1.amazonaws.com/guide/univariate/cartograms/)". A [cartogram](https://www.r-graph-gallery.com/cartogram.html) is a map in which the geometry of regions is distorted in order to convey the information of an alternate variable. The region area will be inflated or deflated according to its numeric value. There are generally two types of Cartograms - contiguous (where sizes are distorted but units remain attached to neighbors) and non-contiguous.  

One of the major limitation of Cartogram, however, is that readers who are not familiar with what the areas should look like will not be surprised by the intentional distortion. While this can be somewhat mitigated by having a base map underneath the area cartogram, there is a second limitation: viewers are generally unable to judge sizes well (as mentioned above, same issue as Proportional Symbol), making cartogram hard to interpret accurately.  

**Choropleth**

On the opposite spectrum of popularity, [Choropleth](http://www.axismaps.com.s3-website-us-east-1.amazonaws.com/guide/univariate/choropleth/) is one of the most popular geospatial visualization. Choropleth are used when data:  

1. are attached to enumeration units (e.g. state, city)
2. are standardized to show rates or ratios (not to be used with raw data or counts)
3. have a continuous statistical surface  

In the context of the Airbnb listing data, both criteria 1 and 3 are fulfilled. Criteria 2 is also satisfied when we look at variables such as host acceptance rate, host response rate, or even ratings (which has an upper limit of 100, similar to standardized percentage points).  

While criteria 2 appears to be violated when we explore raw data such as listing counts or price, a classed Choropleth map can be considered to split continuous variable of interest. Classification methods (e.g. natural breaks, equal interval, quantile) and number of classes, in this case, can be adjusted to suit contextual needs. In fact, classed Choropleth has the additional advantage of allowing viewers to easily "get numbers off the map". While classification introduces subjectivity and a form of aggregation, these disadvantages are worthwhile trade-offs for the ease of making accurate and simple comparison across neighboring area, while not losing objective truth (ultimately, the data points are still within a range within the class - in fact, some times this provides an even more telling story than an unclassed version would).  

In fact,  with the aggregation within area borders and easy comparison across areas, choropleth provides a good foundation to what we are going to discuss in the next segment, Geospatial Analysis.    

All in all, in the context of the Airbnb listing data, we will see in our iterations of different visualizations that dot density and proportional symbols are not suitable candidates for practical reasons (hint: they are concentrated in certain popular areas with many overlapping points). Cartogram, while plausible, is disadvantaged by the limitations we have discussed above, and is less intuitive when mapped against geospatial cluster analysis (discussed below).  

By the ease of interpretation and the applicability for geospatial analysis, the Choropleth is, on theory, the visualization of choice. We will further explore its practicality below when we conduct the actual viz iterations using the listing data.  


## 2.2 Sub-module 2: Geospatial Analysis  

Geospatial analysis consists of a wide spectrum of topics including spatial autocorrelation, spatio-temporary data analysis, spatial regression, surface analysis, flow analysis, network and locational analysis, and even other geocomputational methods.  

In the context of the Airbnb listing data, given that listings are static data, we will eliminate methods focusing on flow analyses. Temporal analysis, while plausible (e.g. how listings or prices change over time) and provided within the datasets, are not the main scope of this study, which aims to provide analytical tools to understand geospatial distribution of various variable of interest. In this aspect, exploratory spatial data analysis (ESDA) that includes spatial autocorrelation, looking at indicators for spatial association would be more closely applicable.  

One common ESDA methods (and indeed, objective) is to conduct geographical clustering, to identify areas with exceptionally high or low value of variables of interest, as well as outliers where there are high regions surrounded by low areas or low regions surrounded by high areas.In this context, [indicators of spatial association](https://en.wikipedia.org/wiki/Indicators_of_spatial_association), or "*statistics that evaluate the existence of clusters in the spatial arrange of a given variable* will be used as a measure to test for existence of clusters.  

Global spatial analysis or global spatial autocorrelation analysis assumes homogeneity across the area studied, and yields statistic to summarize the whole study area. On the other hand, local spatial analysis, even in the absence of global autocorrelation, allows identification of clusters at local level (i.e. in sub-'regions') using local spatial autocorrelation.  

Conventional analysis in this aspect - the Hot Spot Analysis - typically adopts [Getis-Ord's G and G* statistics](https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1538-4632.1995.tb00912.x) which provides hot/cold spot analysis within a geographical area based testing of statistical significance that rejects (or fail to reject) the null hypothesis that there are no hot/cold spots vis-a-vis its neighbouring features.  

[Anselin (1995)](https://onlinelibrary.wiley.com/doi/epdf/10.1111/j.1538-4632.1995.tb00338.x) first provided the theoretical discussion on another similar approach: the **Local Indicators of Spatial Association (LISA)** statistic. In her definition, she explained that a LISA is "*any statistic that satisfies the following two requirements:*  

*a. the LISA for each observation gives an indication of the event of significant spatial clustering of similar values around that observation;*  
*b. the sum of LISAs for all observations is proportional to a global indicator of spatial association*"  

Anselin explained in her extract that LISA would serve two purposes:  

1. On the one hand they may be interpreted as indicators of local pockets of non-stationarity (i.e. hot spots); and   
2. On the other, they may be used to to assess influence of individual locations on the magnitide of the global statistics and hence to identify 'outliers'  

For 1, LISA can be used as the basis for a test of the null hypothesis of no local spatial association (similar to the G- and G*-statistics of Getis and Ord (1992))  

For 2, If the underlying variable is stable throughout the data, then local indications will show little variation around the global statistic (which is representative of the average pattern of local association) - local values that are very different from the global value, hence, would indicate locations that contribute more than their expected share to the global statistics, indicating likely outliers. This interpretation of LISA overcomes some of the earlier challenges of statistics (e.g. Getis-Ord G & G*) that are applicable only when global statistics do not provide evidence of spatial association - however, a strong and significant indication of global spatial association may still hide totally random subset (including possible distinct local patterns).  

Within the wider class of LISA statistics, Anselin discussed and adopted the use of the local Moran I indices for the local spatial analysis.  

While [Hot Spot Analysis using the Getis-Ord G*](https://pro.arcgis.com/en/pro-app/latest/tool-reference/spatial-statistics/h-how-hot-spot-analysis-getis-ord-gi-spatial-stati.htm) is effective for finding hot and cold sports, only the [Cluster and Outlier Analysis using Anselin's Local Moran I](https://pro.arcgis.com/en/pro-app/latest/tool-reference/spatial-statistics/h-how-cluster-and-outlier-analysis-anselin-local-m.htm) will identify statistically significant spatial outliers.  

This paper, in Section 4.2, provides a prototype of these analysis, along with some testing of different parameters that can be interactively altered (e.g. methods for determining neighbour) in a web-based application.  

We next proceed to get our hands dirty and conduct some data wrangling and preparation for the prototypes of the sub-modules.  


# 3: Data Extracting, Wrangling, and Preparation  

This Section provides a step-by-step explanation on the data preparation process.  

**Step 1: Load required packages ** 
```{r}
packages = c('tidyverse','stringr', 'dplyr', 'sf', 'ggplot2', 'leaflet', 'tmap', 'cartogram', 'spdep', 'RColorBrewer', 'colorspace')
for (p in packages){
  if(!require(p,character.only = T)){
    install.packages(p)
  }
  library(p,character.only = T)
}
```

**Step 2: Extract data from CSV file** 
```{r}
airbnb_raw <- read_csv("data/listings.csv")
```

**Step 3: Select relevant columns for the project** 
```{r}
airbnb <- select(airbnb_raw, c(1, 6, 7, 8, 10, 13, 14, 15, 16, 17, 18, 19, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 49, 50, 51, 52, 53, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 80))
```

**Step 4a: Convert Date Variables from chr to date Type**
```{r}
airbnb$host_since <- as.Date(airbnb$host_since, '%d/%m/%y')
airbnb$first_review <- as.Date(airbnb$first_review, '%d/%m/%y')
airbnb$last_review <- as.Date(airbnb$last_review, '%d/%m/%y')
```

**Step 4b: Convert Rate Variables from chr to dbl Type**
```{r}
airbnb$host_response_rate <- ((as.numeric(str_remove_all(airbnb$host_response_rate, "%")))/100)
airbnb$host_acceptance_rate <- ((as.numeric(str_remove_all(airbnb$host_acceptance_rate, "%")))/100)
```

**Step 4c: Converting Price Variables from chr to dbl Type**
```{r}
airbnb$price <- as.numeric(gsub("\\$","", airbnb$price))
```

**Step 4d: replacing String "N/A"s with NA**
```{r}
airbnb$host_response_time <- na_if(airbnb$host_response_time, "N/A")
```

**Step 4e: Save dataset as "Airbnb.csv"**
```{r, eval=FALSE}
# Note: this code is not ran
write.csv(airbnb, "data/Airbnb.csv")
```

This preliminary cleaned *Airbnb.csv* file is then distributed to all team members for further wrangling to suit the needs of respective sub-modules. For example, text analytics will require tokenization of character strings, geospatial requires joining with spatial files to form geospatial dataframes, and predictive analytics requires further coorelation analysis before deciding on predictors to retain.  

For the purpose of this assignment, the data wrangling process from Step 5 below onwards will focus on creating spatial simple features and other processing methods for the geospatial visualization and analysis sub-modules.  

**Step 5: Select relevant variables and creating spatial feature (sf) dataframe**  

Firstly, the *st_as_sf* code is ran using the *longitude* and *latitude* columns that were scraped and provided in the original data file as the coordinates to match the points. *crs* is set to 4326, corresponding to the Australia map boundary. This sf file containing all the airbnb listings data is named ***airbnb_sf***.  

Next, long string variables (e.g. description, house rules) that are more relevant for text analytics are removed to reduce size of file to be converted into sf, giving rise to the ***aust_sf*** dataframe.  

```{r}
airbnb_sf <- st_as_sf(airbnb,coords = c("longitude","latitude"), crs=4326)
aust_sf <- select(airbnb_sf, c(1:2,5:6,9:14,16:24,26:28,35,40,43:45,53,58:63))
```

**Step 6: Download & Import Local Government Area (LGA) boundaries Polygon**  

The Australian Statistical Geography Standard (ASGS) [Local Government Areas (LGAs)](https://www.abs.gov.au/ausstats/abs@.nsf/Lookup/by%20Subject/1270.0.55.003~June%202020~Main%20Features~Local%20Government%20Areas%20(LGAs)~3) are an ABS approximation of officially gazetted Local Government Areas as defined by each State and Territory Local Government Department. LGAs cover incorporated areas of Australia only, and are legally designated parts of a State or Territory over which incorporated local governing bodies have responsibility.  

Inside AirBnb's dataset includes *region_name* and *region_id* fields which corresponds to the LGAs. Geospatial polygons files of the LGAs are regularly updated and provided by the Australian Bureau of Statistics. The geospatial files can be downloaded via ths [ link](https://www.abs.gov.au/AUSSTATS/abs@.nsf/DetailsPage/1270.0.55.003June%202020?OpenDocument) and selecting the *"Local Government Areas ASGS Ed 2020 Digital Boundaries in ESRI Shapefile format"* file.  

After moving the extracted file into the data folder of this Markdown directory, we run the following codes to extract the map layer as a ***aust_lga*** dataframe:  

```{r}
aust_lga <- st_read(dsn = "data/geospatial", 
                layer = "LGA_2020_AUST")
```

**Step 7: Aggregate vairables to the LGA level**

On the trimmed *airbnb* file, we select relevant fields that makes sense to be aggregated into the LGA level. These includes:  

- ***listing_count***: summing up the number of listings within the LGA
- ***median_price***: median price of all listings within the LGA
- ***average_review***: average number of reviews for all listings within the LGA
- ***average_rating***: average rating across all listings within the LGA
- ***average_response_rate***: average response rate across all listings within the LGA
- ***average_acceptance_rate***: average acceptance rate across all listings within the LGA

Other variables such number of beds/bedrooms, maximum/minimum nights, and other date and Boolean/logical fields are not meaningful when aggregated together on the LGA level. This aggregated dataframe is named ***aust_grp***  

```{r}
aust_grp <- airbnb %>%
  group_by(region_id) %>%
  summarise(listing_count=n(),
            median_price=median(price, na.rm=T),
            average_review=mean(number_of_reviews, na.rm=T),
            average_rating=mean(review_scores_rating, na.rm=T),
            average_response_rate=mean(host_response_rate, na.rm=T),
            average_acceptance_rate=mean(host_acceptance_rate, na.rm=T))

```

**Step 8: Join Aggregated variables with LGA Shapefile**  

Each LGA has its corresponding standardized code, which is captured by the region_id field in the Inside Airbnb dataset. First we change the region_id datatype to character, then we join the two files (LGA-aggregated data and the LGA shapefile) using the left_join function to create the ***aust_grp_sf*** simple feature (sf) dataframe.  

We also further remove any empty rows with no available data for the LGA - creating the ***aust_sf_clean*** dataframe.  

```{r}
aust_grp$region_id <- as.character(aust_grp$region_id)
aust_grp_sf <-left_join(aust_lga, aust_grp, by = c("LGA_CODE20"="region_id"))
aust_sf_clean <- aust_grp_sf[!st_is_empty(aust_grp_sf),]
```

**Step 9: State Filter - Victoria**  

As the resultant files are big, we filter out the data set to focus on just the Victoria state. This provides an illustration on how the web-based application could look like when an additional State filter is applied.  

*Note: field region_parent_name corresponds to the 9 states in Australia; each of the state also has a fixed numerical code from 1-9 for each of the 9 states, in this case, the code for Victoria is "2"*  

***vic_sf*** contains all the data from the aust_sf sf dataframe filtered down to the Victoria state   
***vic_grp_sf*** contains LGA-aggregated data for LGAs within the Victoria state   
***vic_sf_clean*** contains non-empty LGA-aggregated data for LGAs within the Victoria state  

```{r}
vic_sf <- filter(aust_sf, (aust_sf$region_parent_name=="Victoria"))
vic_grp_sf <- filter(aust_grp_sf, (aust_grp_sf$STE_CODE16=="2"))
vic_sf_clean <- vic_grp_sf[!st_is_empty(vic_grp_sf),]
```

**Step 10: Final Data Preparation based on Variable of Interest**  

And finally, before we proceed with the testing, we conduct a second-level cleaning that removes NA values specific to the variable of interest. In this paper, we will be focusing on listing price as the variable of interest, and hence, we conduct this second-level cleaning based on rows where prices are "NA". 

Here we create *aust_sf_clean2* where second-level cleaning is conducted to remove rows where median_price are NA.  
We then conduct the same filtering to extract data for the Victoria state to create the *vic_sf_clean2* dataframe  
We keep the file size even smaller, by extracting just the LGA name, median price, and geometry data in *vic_median_price*  

```{r}

# Second-level further cleaning to remove rows where median price (variable of interes) is NA 
aust_sf_clean2 <- aust_sf_clean[!is.na(aust_sf_clean$median_price),]
vic_sf_clean2 <- filter(aust_sf_clean2, (aust_sf_clean2$STE_CODE16=="2"))
vic_median_price <-  select(vic_sf_clean2, c(2,7,12))

```

