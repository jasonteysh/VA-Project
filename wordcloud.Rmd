## load required R packages
```{r}
packages = c('tidyverse', 'tidytext', 'tm', 'SnowballC', 'wordcloud', 'wordcloud2', 'RColorBrewer', 'stringi', 'readr', 'stringr', 'topicmodels', 'ggplot2', 'dplyr', 'plotly', 'parallelPlot', 'readtext', 'quanteda', 'stringi', 'sentimentr', 'SentimentAnalysis', 'syuzhet', 'RSentiment')
for (p in packages){
if(!require(p, character.only = T)){
install.packages(p)
}
library(p, character.only = T)
}
```

## read in Airbnb.csv
```{r}
airbnb <- read_csv("data/Airbnb(extract100).csv")
```

## tokenisation to create a tibble of text from 'description' column
```{r}
airbnb_text <- airbnb$description
airbnb_tibble <- tibble(airbnb_text)
airbnb_tibble %>%
  unnest_tokens(word, airbnb_text)
```

## Create a corpus
```{r}
airbnb_corpus <- Corpus(VectorSource(airbnb_tibble))

# cleaning the corpus
airbnb_corpus_clean <- tm_map(airbnb_corpus, stripWhitespace)
airbnb_corpus_clean <- tm_map(airbnb_corpus_clean, content_transformer(tolower))
airbnb_corpus_clean <- tm_map(airbnb_corpus_clean, removeNumbers)
airbnb_corpus_clean <- tm_map(airbnb_corpus_clean, removePunctuation)
airbnb_corpus_clean <- tm_map(airbnb_corpus_clean, removeWords, c((stopwords("english")), "%br"))
airbnb_corpus_clean <- tm_map(airbnb_corpus_clean, stemDocument)
```
#create the Document Term Matrix
```{r}
airbnb_corpus_dtm <- DocumentTermMatrix(airbnb_corpus_clean)
```

#create the wordcloud
```{r}
sums <- as.data.frame(colSums(as.matrix(airbnb_corpus_dtm)))
sums <- rownames_to_column(sums) 
colnames(sums) <- c("terms", "count")
sums <- arrange(sums, desc(count))
head <- sums

#wordcloud will show the most common terms used in the 'description' column
wordcloud(words = head$terms, freq = head$count, min.freq = 10,
  max.words=200, random.order=FALSE, rot.per=0.35, 
  colors=brewer.pal(8, "Dark2"))
```

## Topic modelling using Latent Dirichlet Allocation (LDA) model
```{r}
# set a seed so that the output of the model is predictable
airbnb_lda <- LDA(airbnb_corpus_dtm, k = 5, control = list(seed = 1234))
#> A LDA_VEM topic model with 5 topics.
```

## extracting the per-topic-per-word probabilities, called Î² (beta)
```{r}
airbnb_topics <- tidy(airbnb_lda, matrix = "beta")
```

## find the 10 most common terms within each topic
```{r}
airbnb_top_terms <- airbnb_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

airbnb_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()
```

## sentiment analysis
```{r}
sent <- analyzeSentiment(airbnb_dtm, language = "english")
# we're going to just select the Harvard-IV dictionary results ..  
sent <- sent[,1:4]
#Organizing it as a dataframe
sent <- as.data.frame(sent)
# Now lets take a look at what these sentiment values look like. 
view(head(sent))

summary(sent$SentimentGI)

# Start by attaching to other data which has the company names 
final <- bind_cols(airbnb_dtm$terms, sent)
# now lets get the top 5 
final %>% group_by(airbnb_dtm$terms) %>%
  summarize(sent = mean(SentimentGI)) %>%
  arrange(desc(sent)) %>%
  head(n= 5)


sent2 <- get_nrc_sentiment(airbnb$description)
# Let's look at the corpus as a whole again:
sent3 <- as.data.frame(colSums(sent))
sent3 <- rownames_to_column(sent3) 
colnames(sent3) <- c("emotion", "count")
ggplot(sent3, aes(x = emotion, y = count, fill = emotion)) + geom_bar(stat = "identity") + theme_minimal() + theme(legend.position="none", panel.grid.major = element_blank()) + labs( x = "Emotion", y = "Total Count") + ggtitle("Sentiment of Job Descriptions") + theme(plot.title = element_text(hjust=0.5))
```

#LDA analysis
```{r}
# set a seed so that the output of the model is predictable
ap_lda <- LDA(airbnb_dtm, k = 2, control = list(seed = 1234))
ap_lda
#> A LDA_VEM topic model with 2 topics.
```

```{r}
airbnb_top_terms <- airbnb_topics %>%
  group_by(term) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(term, -beta)

airbnb_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()
```